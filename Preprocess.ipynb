{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Importation & fields selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding data/Tweets.txt it becomes 10005 lines.\n",
      "After adding data/training_dataset.csv it becomes 11021 lines.\n",
      "After adding ./data/tweets_folder/generated/ArTwitter.csv it becomes 12972 lines.\n",
      "After adding ./data/tweets_folder/generated/ASTD.csv it becomes 14641 lines.\n",
      "After adding ./data/tweets_folder/generated/qrci.csv it becomes 15395 lines.\n",
      "After adding Tweets by files in tweets_folder it becomes 17386 lines.\n",
      "After adding ./data/G2.csv it becomes 23768 lines.\n",
      "After adding ./data/LABR-book-reviews.csv it becomes 40216 lines.\n",
      "After removing duplicates it becomes 35801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text\n",
       "           count\n",
       "sentiment       \n",
       "0          11317\n",
       "1          11668\n",
       "2          12816"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "def nothing(name,tab):\n",
    "    print('After adding '+name+' it becomes '+str(len(tab))+' lines.')\n",
    "s = pd.read_csv(\"data/Tweets.txt\",delimiter='\\t')\n",
    "s.columns = ['text','sentiment']\n",
    "s_imroved = []\n",
    "# Source Data Problems must solve\n",
    "for index, row in s.iterrows():\n",
    "    text = row['text'].split('\\n')\n",
    "    text_by_t = row['text'].split('\\t')\n",
    "    if len(text)>1:\n",
    "        for t in text:\n",
    "            r = t.split('\\t')\n",
    "            s_imroved.append(r)\n",
    "    else:\n",
    "        pass\n",
    "        s_imroved.append([row['text'],row['sentiment']])\n",
    "nothing('data/Tweets.txt',s_imroved)\n",
    "\n",
    "\n",
    "s = pd.read_csv(\"data/training_dataset.csv\",delimiter=',')\n",
    "s.columns = ['text','sentiment']\n",
    "for index, row in s.iterrows():\n",
    "    s_imroved.append([row['text'],row['sentiment']])\n",
    "nothing('data/training_dataset.csv',s_imroved)    \n",
    "s = pd.read_csv(\"./data/tweets_folder/generated/ArTwitter.csv\",delimiter=',')\n",
    "s.loc[s.sentiment == 0,'sentiment'] = 2\n",
    "for index, row in s.iterrows():\n",
    "    s_imroved.append([row['txt'],row['sentiment']])\n",
    "nothing('./data/tweets_folder/generated/ArTwitter.csv',s_imroved)\n",
    "\n",
    "s = pd.read_csv(\"./data/tweets_folder/generated/ASTD.csv\",delimiter=',')\n",
    "s.loc[s.sentiment == 0,'sentiment'] = 2\n",
    "for index, row in s.iterrows():\n",
    "    \n",
    "    text = row['txt'].split('\\t')\n",
    "    if len(text)>1:\n",
    "        for t in text:\n",
    "            r = t.split('\\t')\n",
    "            s_imroved.append(r)\n",
    "    else:\n",
    "        pass\n",
    "        s_imroved.append([row['txt'],row['sentiment']])\n",
    "\n",
    "nothing('./data/tweets_folder/generated/ASTD.csv',s_imroved)\n",
    "\n",
    "    \n",
    "s = pd.read_csv(\"./data/tweets_folder/generated/qrci.csv\",delimiter=',')\n",
    "s.loc[s.sentiment == 0,'sentiment'] = 2\n",
    "for index, row in s.iterrows():\n",
    "    s_imroved.append([row['txt'],row['sentiment']])\n",
    "nothing('./data/tweets_folder/generated/qrci.csv',s_imroved)\n",
    "\n",
    "    \n",
    "negative = './data/tweets_folder/Negative/'\n",
    "positive = './data/tweets_folder/Positive/'\n",
    "\n",
    "neg_files = os.listdir(negative)\n",
    "pos_files = os.listdir(positive)\n",
    "\n",
    "df = []\n",
    "def get_lines(files,label,path,df = []):\n",
    "    for f in files:\n",
    "        try:\n",
    "            line = open(path+f,encoding='utf-8').readline()\n",
    "            df.append([line.strip('\\r\\n\\t'),label])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "s_imroved = get_lines(neg_files,2,negative,s_imroved)\n",
    "s_imroved = get_lines(pos_files,1,positive,s_imroved)\n",
    "nothing('Tweets by files in tweets_folder',s_imroved)\n",
    "s = pd.read_csv(\"./data/G2.csv\",delimiter=',',error_bad_lines=False,warn_bad_lines=False)\n",
    "s = s[['norm-word_Ar','sentiment']]\n",
    "\n",
    "s.loc[s.sentiment == 'Negative','sentiment'] = 2\n",
    "s.loc[s.sentiment == 'Positive','sentiment'] = 1\n",
    "s.loc[s.sentiment == 'Neutral','sentiment'] = 0\n",
    "for index, row in s.iterrows():\n",
    "    s_imroved.append([row['norm-word_Ar'],row['sentiment']])\n",
    "nothing('./data/G2.csv',s_imroved)\n",
    "\n",
    "s = pd.read_csv(\"./data/LABR-book-reviews.csv\",delimiter=',')\n",
    "s.loc[s.sentiment == 0,'sentiment'] = 2\n",
    "for index, row in s.iterrows():\n",
    "    s_imroved.append([row['txt'],row['sentiment']])\n",
    "nothing('./data/LABR-book-reviews.csv',s_imroved)\n",
    "s = pd.DataFrame(s_imroved)\n",
    "s.columns = ['text','sentiment']\n",
    "\n",
    "s.loc[s.sentiment == 'POS','sentiment'] = 1\n",
    "s.loc[s.sentiment == 'NEUTRAL','sentiment'] = 0\n",
    "s.loc[s.sentiment == 'OBJ','sentiment'] = 0\n",
    "s.loc[s.sentiment == 'NEG','sentiment'] = 2 \n",
    "\n",
    "df = s.loc[s.sentiment == 1]\n",
    "df = df.append(s.loc[s.sentiment == 0])\n",
    "df = df.append(s.loc[s.sentiment == 2])\n",
    "train = df.drop_duplicates()\n",
    "print(\"After removing duplicates it becomes \"+str(len(train)))\n",
    "original_dasatet = train.copy()\n",
    "train.groupby(['sentiment']).agg(['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_char_per_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36614</th>\n",
       "      <td>\"الحب هو ما حدث بيننا. والأدب هو كل ما لم يحد...</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>19088</td>\n",
       "      <td>4.931324</td>\n",
       "      <td>781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26416</th>\n",
       "      <td>النسيان أحلام مستغانمى 1* أجيبه كما لم تحب إم...</td>\n",
       "      <td>1</td>\n",
       "      <td>2915</td>\n",
       "      <td>14773</td>\n",
       "      <td>4.071061</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29661</th>\n",
       "      <td>قراءتي للكتاب وهي لا تغني عن قراءة الكتاب بال...</td>\n",
       "      <td>1</td>\n",
       "      <td>2138</td>\n",
       "      <td>10840</td>\n",
       "      <td>4.074438</td>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35698</th>\n",
       "      <td>حقيقة أنهيت قراءة كتاب سيّد الأكثر شهرة . ولا...</td>\n",
       "      <td>2</td>\n",
       "      <td>1537</td>\n",
       "      <td>8599</td>\n",
       "      <td>4.601303</td>\n",
       "      <td>355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39703</th>\n",
       "      <td>1* كلُّ شيءٍ ممكنٌ . حينَ يكون المرءُ بالكوني...</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>7777</td>\n",
       "      <td>4.190921</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "36614   \"الحب هو ما حدث بيننا. والأدب هو كل ما لم يحد...         1   \n",
       "26416   النسيان أحلام مستغانمى 1* أجيبه كما لم تحب إم...         1   \n",
       "29661   قراءتي للكتاب وهي لا تغني عن قراءة الكتاب بال...         1   \n",
       "35698   حقيقة أنهيت قراءة كتاب سيّد الأكثر شهرة . ولا...         2   \n",
       "39703   1* كلُّ شيءٍ ممكنٌ . حينَ يكون المرءُ بالكوني...         2   \n",
       "\n",
       "       word_count  char_count  avg_char_per_word  stopwords  emoji_count  \n",
       "36614        3220       19088           4.931324        781            0  \n",
       "26416        2915       14773           4.071061        569            0  \n",
       "29661        2138       10840           4.074438        528            0  \n",
       "35698        1537        8599           4.601303        355            0  \n",
       "39703        1500        7777           4.190921        211            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "#Stats about Text\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def emoji_counter(sentence):\n",
    "    return emoji.emoji_count(sentence)\n",
    "\n",
    "train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train['char_count'] = train['text'].str.len() ## this also includes spaces\n",
    "train['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\n",
    "stop = stopwords.words('arabic')\n",
    "train['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\n",
    "train = train.sort_values(by='word_count',ascending=[0])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from dsaraby import DSAraby\n",
    "ds = DSAraby()\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "stops = set(stopwords.words(\"arabic\"))\n",
    "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
    "ArListem = ArabicLightStemmer()\n",
    "\n",
    "\n",
    "def to_arabic(text):\n",
    "    return ds.transliterate(text)\n",
    "\n",
    "def stem(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ArListem.light_stem(w)\n",
    "        cleaned.append(ArListem.get_root())\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return araby.strip_tashkeel(text)\n",
    "    \n",
    "def remove_stop_words(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texts from Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with Hashtags in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with emojis in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "import emoji\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('@[^\\s]+',' ',text)\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Clean for tweets\n",
    "    text = clean_tweet(text)\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    ## Remove Emojis\n",
    "    text = remove_emoji(text)\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "    ## Arabisy the text\n",
    "    text = to_arabic(text)\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    text = re.sub('[A-Za-z]+',' ',text)\n",
    "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    #Stemming\n",
    "    #text = stem(text)\n",
    "    return text\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Common word removal\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[:12]\n",
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "### Rare words removal\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[-50:]\n",
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "### Remove Numbers from text\n",
    "\n",
    "\n",
    "### Discouver Data again after cleaning\n",
    "train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train['char_count'] = train['text'].str.len() ## this also includes spaces\n",
    "train['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\n",
    "stop = stopwords.words('arabic')\n",
    "train['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\n",
    "train = train.sort_values(by='word_count',ascending=[0])\n",
    "final = []\n",
    "for index, row in train.iterrows():\n",
    "    if len(row['text'].split()) > 3:\n",
    "        final.append([row['text'],row['sentiment']])\n",
    "df = pd.DataFrame(final)\n",
    "df.columns = ['text','sentiment']\n",
    "df.to_csv('final_nice.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الحب حدث بيننا والادب يحدث هنيءا للادب فجيعتنا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>النسيان احلام مستغانمي اجيبه تحب امراه وانسيه ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قراءتي للكتاب تغني قراءه بالمره يدعو لاعاده قر...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ريفيو القراءه مطلع سنه اعمال نجيب محفوظ محبتها...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حقيقه انهيت قراءه سيد الاكثر شهره يبدو انني ات...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  الحب حدث بيننا والادب يحدث هنيءا للادب فجيعتنا...          1\n",
       "1  النسيان احلام مستغانمي اجيبه تحب امراه وانسيه ...          1\n",
       "2  قراءتي للكتاب تغني قراءه بالمره يدعو لاعاده قر...          1\n",
       "3  ريفيو القراءه مطلع سنه اعمال نجيب محفوظ محبتها...          1\n",
       "4  حقيقه انهيت قراءه سيد الاكثر شهره يبدو انني ات...          2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_char_per_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39551</th>\n",
       "      <td>الطريق</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28081</th>\n",
       "      <td>معجبنيش</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>نور</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33990</th>\n",
       "      <td>راءعه</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11511</th>\n",
       "      <td>هبل</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text sentiment  word_count  char_count  avg_char_per_word  \\\n",
       "39551   الطريق         2           1           6                6.0   \n",
       "28081  معجبنيش         2           1           7                7.0   \n",
       "7312       نور         1           1           3                3.0   \n",
       "33990    راءعه         1           1           5                5.0   \n",
       "11511      هبل         2           1           3                3.0   \n",
       "\n",
       "       stopwords  emoji_count  \n",
       "39551          0            0  \n",
       "28081          0            0  \n",
       "7312           0            0  \n",
       "33990          0            0  \n",
       "11511          0            0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
